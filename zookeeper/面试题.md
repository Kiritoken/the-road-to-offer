# 面试题

https://zhuanlan.zhihu.com/p/348753812



## 节点状态有哪些
+ looking状态，竞选状态
+ following，随从状态，同步leader状态，参与投票
+ observing，观察者状态，同步leader状态，不参与投票
+ leading，领导者状态


## 初始化选举和崩溃选举过程
假设有5个节点
+ 初始化选举过程
    + 节点1先启动，根据zxid和myid投给自己
    + 节点2启动，也投给自己，并广播投票信息
    + 节点1收到2的投票，也转投给2
    + 节点3投给自己，1、2都投给3，超过半数，3成为领导者
    + 节点4也投票给3
    + 节点5同样投给3


+ leader挂掉了，崩溃选举过程


* 1. 投票给自己
* 2. 把选票结果发送到投票队列里，由线程消费，发送给每一个节点
* 3. 从收票队列里取出其他节点的选票
* 4. 如果队列为空，看是否需要与其他客户端建立连接，一般只能myid小的连接myid大的
* 5. 与选票队列里获取的选票进行pk，更新自己的选票
* 6. 保存各个节点的选票信息，使用map保存<myid,Vote>
* 7. 统计投票信息
* 8. 与本节点投票相同的节点数量结果已经过半，更新状态




## zk数据模型
znode 
acl
***临时节点不能有子节点***


## zk的数据同步原理
https://blog.csdn.net/weixin_33826609/article/details/91389764
3个重要的判断数据：
+ peerLastZxid follower最后的事务id
+ minCommittedLog leader提交的最小的事务id
+ maxCommittedLog leader提交的最大的事务id
主要几种情况如下：
+ DIFF 差异化同步
    + peerLastZxid == maxCommittedLog 发送DIFF包，同步的事务为peerLastZxid，即无需同步
    + minCommittedLog< peerLastZxid< maxCommittedLog 发送DIFF包，同步没有的事务

+ TRUNC+DIFF 回滚+差异化同步
虽然 follower 的 peerLastZxid 介于 maxCommittedLog， minCommittedLog 两者之间，但是 follower 的 peerLastZxid 在 leader 节点中不存在
***此时 leader 需告知 follower 先回滚到 peerLastZxid 的前一个 zxid, 回滚后再进行差异化同步***


+ TRUC 回滚同步
若 follower 的 peerLastZxid 大于 leader 的 maxCommittedLog，则告知 follower 回滚至 maxCommittedLog； 该场景可以认为是 TRUNC+DIFF 的简化模式

+ SNAP全量同步
peerLastZxid < minCommittedLog时进行全量同步，leader发送SNAP报文，后直接将内存数据库全量的数据序列化后发送给follwer，follwer收到后再反序列化到自己的内存数据库



## watcher监听机制实现原理
分布式环境下的观察者模式
+ 客户端
ZKWatchManager

对应的String就是监听的路径
```java
    private final Map<String, Set<Watcher>> dataWatches = new HashMap<>();
    private final Map<String, Set<Watcher>> existWatches = new HashMap<>();
    private final Map<String, Set<Watcher>> childWatches = new HashMap<>();
    private final Map<String, Set<Watcher>> persistentWatches = new HashMap<>();
    private final Map<String, Set<Watcher>> persistentRecursiveWatches = new HashMap<>();
```
服务端watch是一次性的，服务端在查询到watch之后会将其从中删除，调用process方法向客户端发送通知，封装的一个watchevent对象

客户端watch也是一次性的

watch一定是一个一个调用的，因为内部是一个FIFO的队列，不用担心并发问题

客户端收到watchevent时间后，也会在自己本地的WatchManager中查找是否存在Watcher，触发后就删除


+   client在发送请求时候，会将watch的具体状态保存在client中，即存在于等待回复队列中
+   标记watch的request到达服务端后，服务端会将这个watcher（包含client的连接属性）以字典的形式保存在内存中
+   当watch的数据发生相应变化时，去字典里找出注册的watch，并拿到对应client连接
+   根据连接，发送一个通知到client
+   client从等待回复队列中取出元素，watch的回调被触发


## 典型应用场景
+ 配置中心（数据发布/订阅）
+ 负载均衡： 提供服务列表
+ 命名服务：提供服务名到服务地址的映射
+ 集群管理
+ 分布式锁
+ 分布式队列

## zk中一个客户端修改了某个节点的数据，其他客户端能够马上获取到这个最新的数据吗
不能，能获取到老版本的数据
可以先使用sync，再使用getData，能够得到最新数据


## zk对事务性的支持
比如一个操作需要新增某个几点，然后删除节点，多个操作保证原子性
```java
CuratorTransaction transaction = client.inTransaction();  
  
Collection<CuratorTransactionResult> results = transaction.create()  
        .forPath("/a/path", "some data".getBytes()).and().setData()  
        .forPath("/another/path", "other data".getBytes()).and().delete().forPath("/yet/another/path")  
        .and().commit();  
```


## 观察者机制
peertype = observer
动态扩展zookeeper集群又不会降低写性能

+ observer不参与投票，写请求转发给leader
+ observer节点故障不会影响集群可用性
+ ***领导者选举不参与投票，过半不计算observer的数量***

## zk会话管理机制
session
https://blog.csdn.net/SCUTJAY/article/details/106889060

分桶策略
1、

## zab协议  

zookeeper专用的支持崩溃恢复的原子广播协议，***实现分布式数据一致性***
+ 崩溃恢复


+ 消息广播


## zookeeper和eureka的区别

CAP理论的两种实现


+ zookeeper
cp（强一致性）
leader选举时，服务不可用


+ eureka
AP （高可用）
eureka各个节点都是平等的
eureka客户端有缓存
依靠心跳确保服务连接正常


## CAP证明

反证法： 
如果CAP同时满足，如果允许分区容错性的存在，那么一定存在节点直
之间的丢包，那么不能保证一致性

## BASE
+ 基本可用
+ 软状态
+ 最终一致性


## 负载均衡算法
+ 轮询
+ 加权轮询
+ 随机
+ 源地址哈希
+ 最小连接数